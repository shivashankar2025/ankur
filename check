# -------------------------------
# IMPORTS
# -------------------------------

import os
import re
import ssl
import socket
import whois
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse

from dotenv import load_dotenv
import openai
import gradio as gr

# -------------------------------
# ENVIRONMENT SETUP
# -------------------------------

load_dotenv(override=True)
api_key = os.getenv('OPENAI_API_KEY')

# Initialize OpenAI API Key
openai.api_key = api_key


# -------------------------------
# WEBSITE ANALYZER CLASS
# -------------------------------

class WebsiteSummarizer:
    def __init__(self, model="gpt-4o"):
        self.model = model

    def fetch_website_text(self, url):
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            return soup.get_text(separator=' ', strip=True)
        except Exception as e:
            return f"Failed to fetch website: {e}"

    def analyze_content_with_ai(self, text):
        prompt = f"""
Analyze this website content and determine if it shows signs of being a scam or phishing website. 
Flag if there is urgency, login prompts, or poor grammar. 
Answer just 'safe', 'suspicious', or 'fraudulent'.

Content:
{text[:2000]}
"""
        try:
            result = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            decision = result.choices[0].message["content"].lower()
            if "fraud" in decision:
                return "fraudulent"
            elif "suspicious" in decision:
                return "suspicious"
            return "safe"
        except Exception as e:
            return f"OpenAI Error: {e}"

    def summarize_website(self, url):
        text = self.fetch_website_text(url)
        if text.startswith("Failed to fetch"):
            return text, {}

        summary_prompt = f"Summarize this website content:\n\n{text[:2000]}"
        try:
            result = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": summary_prompt}]
            )
            summary = result.choices[0].message["content"]
        except Exception as e:
            summary = f"OpenAI Error: {e}"

        verdict = self.analyze_content_with_ai(text)
        details = self.run_fraud_checks(url, verdict)

        return summary, details

    def run_fraud_checks(self, url, verdict):
        parsed = urlparse(url)
        domain = parsed.netloc

        return {
            "SSL Valid": self.check_ssl(domain),
            "Domain Age": self.check_domain_age(domain),
            "Suspicious URL Pattern": self.is_suspicious_url(url),
            "Redirects >= 3": self.check_redirects(url),
            "Blacklisted": self.check_blacklist(domain),
            "Content Classification": verdict,
            "Hotlinked Resources": self.check_hotlinking(url),
            "Suspicious Forms Detected": self.detect_forms(url),
            "Missing Headers": self.check_security_headers(url),
            "üõ° Final Score": self.generate_score(verdict),
            "üö¶ Verdict": self.generate_verdict(verdict)
        }

    def check_ssl(self, domain):
        try:
            context = ssl.create_default_context()
            with socket.create_connection((domain, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=domain):
                    return True
        except:
            return False

    def check_domain_age(self, domain):
        try:
            w = whois.whois(domain)
            creation = w.creation_date
            if isinstance(creation, list):
                creation = creation[0]
            age = (datetime.now() - creation).days // 30
            return f"{age} months"
        except:
            return "Unknown"

    def is_suspicious_url(self, url):
        return bool(re.search(r"(login|secure|bank|verify)[\W_]", url, re.IGNORECASE))

    def check_redirects(self, url):
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.get(url, headers=headers, allow_redirects=True, timeout=5)
            return len(r.history) >= 3
        except:
            return False

    def check_blacklist(self, domain):
        # Fake implementation; real version can use PhishTank or VirusTotal APIs
        blacklisted_domains = ["example-scam.com", "paypal-login-fake.com"]
        return domain in blacklisted_domains

    def check_hotlinking(self, url):
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.get(url, headers=headers, timeout=5)
            soup = BeautifulSoup(r.text, "html.parser")
            imgs = soup.find_all("img")
            for img in imgs:
                src = img.get("src", "")
                if src.startswith("http") and urlparse(src).netloc != urlparse(url).netloc:
                    return True
            return False
        except:
            return False

    def detect_forms(self, url):
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.get(url, headers=headers, timeout=5)
            soup = BeautifulSoup(r.text, "html.parser")
            forms = soup.find_all("form")
            return len(forms)
        except:
            return 0

    def check_security_headers(self, url):
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.head(url, headers=headers, timeout=5)
            expected = ["X-Content-Type-Options", "Content-Security-Policy", "Strict-Transport-Security"]
            missing = [h for h in expected if h not in r.headers]
            return missing
        except:
            return expected

    def generate_score(self, classification):
        score_map = {"safe": 10, "suspicious": 5, "fraudulent": 1}
        return score_map.get(classification, 0)

    def generate_verdict(self, classification):
        verdict_map = {
            "safe": "‚úÖ Likely Safe",
            "suspicious": "‚ö†Ô∏è Suspicious",
            "fraudulent": "üö´ Fraudulent"
        }
        return verdict_map.get(classification, "‚ùì Unknown")


# -------------------------------
# GRADIO UI SETUP
# -------------------------------

summarizer = WebsiteSummarizer()

def analyze_url(url):
    summary, details = summarizer.summarize_website(url)

    if isinstance(details, dict):
        formatted = "\n".join([f"**{k}:** {v}" for k, v in details.items()])
    else:
        formatted = "No analysis available."

    return summary, formatted


iface = gr.Interface(
    fn=analyze_url,
    inputs=gr.Textbox(label="Enter Website URL"),
    outputs=[
        gr.Textbox(label="Website Summary"),
        gr.Markdown(label="Fraud Check Report")
    ],
    title="üåê Website Analyzer & Fraud Detector",
    description="Get a summary + fraud analysis of any website using GPT + checks."
)

if __name__ == "__main__":
    iface.launch()
