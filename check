# -------------------------------
# IMPORTS
# -------------------------------

import os
import re
import ssl
import socket
import whois
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse
from dotenv import load_dotenv
import openai
import gradio as gr
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
import dns.resolver
from dateutil import parser  # For parsing domain creation date

# -------------------------------
# ENV SETUP
# -------------------------------

load_dotenv(override=True)
api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=api_key)

# -------------------------------
# MAIN CLASS
# -------------------------------

class WebsiteSummarizer:
    def __init__(self, model="gpt-4o"):
        self.client = client
        self.model = model

    def fetch_html_selenium(self, url):
        try:
            options = Options()
            options.add_argument("--headless=new")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            time.sleep(5)
            html = driver.page_source
            driver.quit()
            return html
        except Exception as e:
            print(f"[HTML Fetch Error]: {e}")
            return None

    def check_ssl(self, url):
        try:
            parsed_url = urlparse(url)
            context = ssl.create_default_context()
            with socket.create_connection((parsed_url.hostname, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=parsed_url.hostname) as ssock:
                    ssock.getpeercert()
            return True
        except:
            return False

    def check_domain_age(self, url):
        try:
            domain = urlparse(url).netloc or urlparse(url).hostname
            if domain.startswith("www."):
                domain = domain[4:]
            w = whois.whois(domain)
            creation = w.creation_date

            if isinstance(creation, list):
                creation = [c for c in creation if c]
                creation = creation[0] if creation else None

            if isinstance(creation, str):
                creation = parser.parse(creation)

            if not isinstance(creation, datetime):
                return 0

            now = datetime.now(creation.tzinfo) if creation.tzinfo else datetime.now()
            age_months = (now.year - creation.year) * 12 + (now.month - creation.month)
            return max(0, age_months)
        except Exception as e:
            print(f"[Domain Age ERROR] for {url}: {e}")
            return 0

    def is_url_suspicious(self, url):
        parsed = urlparse(url)
        suspicious = 0
        if re.match(r"\d{1,3}(\.\d{1,3}){3}", parsed.hostname or ""):
            suspicious += 1
        if re.search(r"(login|verify|secure|update)", parsed.hostname or ""):
            suspicious += 1
        if parsed.hostname and parsed.hostname.count('.') > 3:
            suspicious += 1
        if len(url) > 75:
            suspicious += 1
        return suspicious >= 2

    def check_redirects(self, url):
        try:
            r = requests.get(url, allow_redirects=True, timeout=5)
            return len(r.history) >= 3
        except:
            return False

    def check_security_headers(self, url):
        try:
            r = requests.head(url, timeout=5)
            headers = r.headers
            missing = []
            for header in ['X-Content-Type-Options', 'Strict-Transport-Security', 'Content-Security-Policy']:
                if header not in headers:
                    missing.append(header)
            return missing
        except:
            return ['All headers missing or unreachable']

    def check_for_hotlinked_resources(self, soup):
        hotlinks = 0
        for tag in soup.find_all(['img', 'link']):
            src = tag.get('src') or tag.get('href')
            if src and any(trusted in src for trusted in ['google.com', 'facebook.com', 'microsoft.com']):
                hotlinks += 1
        return hotlinks > 2

    def check_forms(self, soup, url):
        forms = soup.find_all('form')
        suspicious_forms = 0
        for form in forms:
            action = form.get('action')
            if action and not action.startswith('https'):
                suspicious_forms += 1
            if any(field.get('name') in ['card', 'cc', 'cvv', 'password']
                   for field in form.find_all(['input', 'textarea'])):
                suspicious_forms += 1
        return suspicious_forms

    def check_minimal_content(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        return len(text.split()) < 50

    def check_blacklists(self, url):
        return False  # Placeholder

    def analyze_content_with_ai(self, text):
        prompt = f"""
Analyze this website content and determine if it shows signs of being a scam or phishing website.
Flag if there is urgency, login prompts, or poor grammar. Answer just 'safe', 'suspicious', or 'fraudulent'.

Content:
{text[:2000]}
"""
        try:
            result = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            decision = result.choices[0].message.content.lower()
            if "fraud" in decision:
                return "fraudulent"
            elif "suspicious" in decision:
                return "suspicious"
            return "safe"
        except:
            return "safe"

    def check_dns_validation(self, url):
        try:
            domain = urlparse(url).hostname
            dns.resolver.resolve(domain, 'A')
            return True
        except:
            return False

    def check_js_behavior(self, html):
        suspicious_keywords = ['eval(', 'document.write', 'setInterval(', 'setTimeout(', 'window.location']
        return any(keyword in html for keyword in suspicious_keywords)

    def check_virustotal_trivy(self, url):
        return "No match found (placeholder)"  # Integration pending

    def summarize(self, url):
        html = self.fetch_html_selenium(url)
        if not html:
            return f"❌ Failed to fetch website: {url}", "❌ Website does not exist or is unreachable."

        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        trimmed_text = text[:4000]

        try:
            chat_response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": f"Summarize the following website content:\n\n{trimmed_text}"}]
            )
            summary = chat_response.choices[0].message.content.strip()
        except Exception as e:
            summary = f"❌ OpenAI API error: {e}"

        # ------------------------------------
        # FRAUD CHECKS
        # ------------------------------------
        score = 0
        issues = []

        ssl_valid = self.check_ssl(url)
        domain_age = self.check_domain_age(url)
        suspicious_url = self.is_url_suspicious(url)
        redirections = self.check_redirects(url)
        blacklist_flag = self.check_blacklists(url)
        header_issues = self.check_security_headers(url)
        hotlinking = self.check_for_hotlinked_resources(soup)
        form_score = self.check_forms(soup, url)
        minimal_content = self.check_minimal_content(html)
        content_status = self.analyze_content_with_ai(trimmed_text)
        dns_valid = self.check_dns_validation(url)
        js_flag = self.check_js_behavior(html)
        vt_result = self.check_virustotal_trivy(url)

        if ssl_valid: score += 1
        if domain_age > 6: score += 1
        if not suspicious_url: score += 1
        if not redirections: score += 1
        if not blacklist_flag: score += 1
        if content_status == "safe": score += 2
        if not hotlinking: score += 1
        if form_score == 0: score += 1
        if len(header_issues) <= 1: score += 1
        if dns_valid: score += 1
        if minimal_content:
            issues.append("Page has very little visible content.")
            score -= 1

        verdict = "Likely Safe ✅" if score >= 7 else "Suspicious ⚠️" if score >= 4 else "Likely Fraudulent ❌"

        report = f"""
### 🔍 Fraud Check Report

**SSL Valid:** {ssl_valid}  
**Domain Age:** {domain_age} months  
**Suspicious URL Pattern:** {suspicious_url}  
**Redirects >= 3:** {redirections}  
**Blacklisted:** {blacklist_flag}  
**Content Classification:** {content_status}  
**Hotlinked Resources:** {hotlinking}  
**Suspicious Forms Detected:** {form_score}  
**Missing Headers:** {", ".join(header_issues)}  
**DNS Valid:** {dns_valid}  
**JS Behavior Suspicious:** {js_flag}  
**Minimal Content:** {minimal_content}  
**VirusTotal/Trivy Scan:** {vt_result}  

**🛡 Final Score:** {score}/11  
**🚦 Verdict:** **{verdict}**
"""
        return summary, report

# -------------------------------
# GRADIO INTERFACE
# -------------------------------

summarizer = WebsiteSummarizer()

iface = gr.Interface(
    fn=summarizer.summarize,
    inputs=gr.Textbox(label="Website URL"),
    outputs=[
        gr.Markdown(label="📝 Website Summary"),
        gr.Markdown(label="🔍 Fraud Analysis Report")
    ],
    title="🌐 Website Summary & Fraud Checker",
    description="Enter a website URL to generate a summary using GPT and scan for DNS, SSL, JS, and fraud indicators."
)

iface.launch()






🔍 Fraud Check Report
SSL Valid: True
Domain Age: 0 months
Suspicious URL Pattern: False
Redirects >= 3: False
Blacklisted: False
Content Classification: safe
Hotlinked Resources: False
Suspicious Forms Detected: 0
Missing Headers: X-Content-Type-Options, Strict-Transport-Security, Content-Security-Policy
DNS Valid: True
JS Behavior Suspicious: False
Minimal Content: True
VirusTotal/Trivy Scan: No match found (placeholder)

🛡 Final Score: 8/11
🚦 Verdict: Likely Safe ✅
