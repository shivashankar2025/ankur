import requests
import whois
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import socket
import ssl
import re

class WebsiteAnalyzer:
    def __init__(self, url):
        self.url = url if url.startswith("http") else "http://" + url
        self.domain = urlparse(self.url).netloc
        self.score = 10
        self.issues = []
        self.html = ""

    def fetch_content(self):
        try:
            # Try normal requests first
            res = requests.get(self.url, timeout=10)
            res.raise_for_status()
            self.html = res.text
        except:
            # Fallback to headless browser
            try:
                chrome_options = Options()
                chrome_options.add_argument("--headless")
                chrome_options.add_argument("--disable-gpu")
                driver = webdriver.Chrome(options=chrome_options)
                driver.get(self.url)
                self.html = driver.page_source
                driver.quit()
            except Exception as e:
                self.issues.append(f"Failed to fetch website content: {e}")
                self.score -= 2
                return

    def check_ssl(self):
        try:
            ctx = ssl.create_default_context()
            with ctx.wrap_socket(socket.socket(), server_hostname=self.domain) as s:
                s.settimeout(3)
                s.connect((self.domain, 443))
            return True
        except:
            self.issues.append("SSL certificate invalid or missing.")
            self.score -= 1
            return False

    def get_domain_age(self):
        try:
            info = whois.whois(self.domain)
            if isinstance(info.creation_date, list):
                created = info.creation_date[0]
            else:
                created = info.creation_date

            if not created:
                raise ValueError("WHOIS data incomplete.")
            age_days = (datetime.now() - created).days
            if age_days < 180:
                self.issues.append(f"Domain is too new: {age_days} days old.")
                self.score -= 1
            return age_days
        except Exception:
            self.issues.append("Domain age could not be determined.")
            self.score -= 1
            return None

    def check_redirects(self):
        try:
            r = requests.get(self.url, timeout=10, allow_redirects=True)
            if len(r.history) >= 3:
                self.issues.append(f"Too many redirects: {len(r.history)}")
                self.score -= 1
        except:
            self.issues.append("Could not verify redirects.")
            self.score -= 1

    def check_security_headers(self):
        try:
            r = requests.get(self.url, timeout=10)
            missing = []
            required = ["X-Content-Type-Options", "X-Frame-Options", "Content-Security-Policy", "Strict-Transport-Security"]
            for header in required:
                if header not in r.headers:
                    missing.append(header)
            if missing:
                self.issues.append(f"Missing security headers: {', '.join(missing)}")
                self.score -= 1
        except:
            self.issues.append("Could not verify headers.")
            self.score -= 1

    def check_minimal_content(self):
        soup = BeautifulSoup(self.html, "html.parser")
        text = soup.get_text(separator=" ", strip=True)
        word_count = len(text.split())
        if word_count < 50:
            self.issues.append("Page has very little visible content.")
            self.score -= 1

    def check_suspicious_url(self):
        if re.search(r"(free|login|bonus|update|gift|win)", self.url, re.IGNORECASE):
            self.issues.append("URL contains suspicious keywords.")
            self.score -= 1

    def check_forms(self):
        soup = BeautifulSoup(self.html, "html.parser")
        forms = soup.find_all("form")
        if forms:
            self.issues.append(f"{len(forms)} form(s) detected on the site.")
            # Not necessarily a deduction

    def analyze(self):
        print(f"\n🔍 Analyzing: {self.url}\n")
        self.fetch_content()
        if not self.html:
            print("❌ Could not analyze website. Skipping.")
            return

        self.check_ssl()
        self.get_domain_age()
        self.check_redirects()
        self.check_security_headers()
        self.check_minimal_content()
        self.check_suspicious_url()
        self.check_forms()

        print(f"\n✅ Final Score: {self.score}/10")
        if self.issues:
            print("\n⚠️ Issues Found:")
            for issue in self.issues:
                print(" -", issue)
        else:
            print("✅ No issues found.")

# Example usage
if __name__ == "__main__":
    url = input("Enter website URL to analyze: ")
    analyzer = WebsiteAnalyzer(url)
    analyzer.analyze()
