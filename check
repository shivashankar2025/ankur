# -------------------------------
# IMPORTS
# -------------------------------

import os
import re
import ssl
import socket
import whois
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse
from dotenv import load_dotenv
import openai
import gradio as gr

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

# -------------------------------
# ENVIRONMENT SETUP
# -------------------------------

load_dotenv(override=True)
api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=api_key)

# -------------------------------
# WEBSITE SUMMARIZER CLASS
# -------------------------------

class WebsiteSummarizer:
    def __init__(self, model="gpt-4o"):
        self.client = client
        self.model = model

    def fetch_html_selenium(self, url):
        try:
            options = Options()
            options.add_argument("--headless=new")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            time.sleep(5)  # wait for JS to load
            html = driver.page_source
            driver.quit()
            return html
        except Exception as e:
            return None

    def check_ssl(self, url):
        try:
            parsed_url = urlparse(url)
            context = ssl.create_default_context()
            with socket.create_connection((parsed_url.hostname, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=parsed_url.hostname) as ssock:
                    cert = ssock.getpeercert()
            return True
        except:
            return False

    def check_domain_age(self, url):
        try:
            domain = urlparse(url).hostname
            whois_info = whois.whois(domain)
            creation_date = whois_info.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]
            if not creation_date:
                return 0
            age_in_months = (datetime.now() - creation_date).days // 30
            return age_in_months
        except:
            return 0

    def is_url_suspicious(self, url):
        parsed = urlparse(url)
        suspicious = 0
        if re.match(r"\d{1,3}(\.\d{1,3}){3}", parsed.hostname or ""):
            suspicious += 1
        if re.search(r"(login|verify|secure|update)", parsed.hostname or ""):
            suspicious += 1
        if parsed.hostname and parsed.hostname.count('.') > 3:
            suspicious += 1
        if len(url) > 75:
            suspicious += 1
        return suspicious >= 2

    def check_redirects(self, url):
        try:
            r = requests.get(url, allow_redirects=True, timeout=5)
            return len(r.history) >= 3
        except:
            return False

    def check_security_headers(self, url):
        try:
            r = requests.head(url, timeout=5)
            headers = r.headers
            missing = []
            if 'X-Content-Type-Options' not in headers:
                missing.append('X-Content-Type-Options')
            if 'Strict-Transport-Security' not in headers:
                missing.append('Strict-Transport-Security')
            if 'Content-Security-Policy' not in headers:
                missing.append('Content-Security-Policy')
            return missing
        except:
            return ['All headers missing or unreachable']

    def check_for_hotlinked_resources(self, soup):
        hotlinks = 0
        for tag in soup.find_all(['img', 'link']):
            src = tag.get('src') or tag.get('href')
            if src and any(trusted in src for trusted in ['google.com', 'facebook.com', 'microsoft.com']):
                hotlinks += 1
        return hotlinks > 2

    def check_forms(self, soup, url):
        forms = soup.find_all('form')
        suspicious_forms = 0
        for form in forms:
            action = form.get('action')
            if action and not action.startswith('https'):
                suspicious_forms += 1
            if any(field.get('name') in ['card', 'cc', 'cvv', 'password'] for field in form.find_all(['input', 'textarea'])):
                suspicious_forms += 1
        return suspicious_forms

    def check_blacklists(self, url):
        return False  # Placeholder

    def analyze_content_with_ai(self, text):
        prompt = f"""
Analyze this website content and determine if it shows signs of being a scam or phishing website. 
Flag if there is urgency, login prompts, or poor grammar. Answer just 'safe', 'suspicious', or 'fraudulent'.

Content:
{text[:2000]}
"""
        try:
            result = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            decision = result.choices[0].message.content.lower()
            if "fraud" in decision:
                return "fraudulent"
            elif "suspicious" in decision:
                return "suspicious"
            return "safe"
        except:
            return "safe"

    def summarize(self, url):
        html = self.fetch_html_selenium(url)
        if not html:
            return f"âŒ Failed to fetch website: {url}", ""

        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        trimmed_text = text[:4000]

        try:
            chat_response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": f"Summarize the following website content:\n\n{trimmed_text}"}]
            )
            summary = chat_response.choices[0].message.content.strip()
        except Exception as e:
            summary = f"âŒ OpenAI API error: {e}"

        # ---------------------
        # FRAUD CHECKS
        # ---------------------

        score = 0
        ssl_valid = self.check_ssl(url)
        domain_age = self.check_domain_age(url)
        suspicious_url = self.is_url_suspicious(url)
        redirections = self.check_redirects(url)
        blacklist_flag = self.check_blacklists(url)
        header_issues = self.check_security_headers(url)
        hotlinking = self.check_for_hotlinked_resources(soup)
        form_score = self.check_forms(soup, url)
        content_status = self.analyze_content_with_ai(trimmed_text)

        if ssl_valid: score += 1
        if domain_age > 6: score += 1
        if not suspicious_url: score += 1
        if not redirections: score += 1
        if not blacklist_flag: score += 2
        if content_status == "safe": score += 2
        if not hotlinking: score += 1
        if form_score == 0: score += 1
        if len(header_issues) <= 1: score += 1

        verdict = "Likely Safe âœ…" if score >= 6 else "Suspicious âš ï¸" if score >= 3 else "Likely Fraudulent âŒ"

        report = f"""
### ğŸ” Fraud Check Report

**SSL Valid:** {ssl_valid}  
**Domain Age:** {domain_age} months  
**Suspicious URL Pattern:** {suspicious_url}  
**Redirects >= 3:** {redirections}  
**Blacklisted:** {blacklist_flag}  
**Content Classification:** {content_status}  
**Hotlinked Resources:** {hotlinking}  
**Suspicious Forms Detected:** {form_score}  
**Missing Headers:** {", ".join(header_issues)}  

**ğŸ›¡ Final Score:** {score}/10  
**ğŸš¦ Verdict:** **{verdict}**
"""
        return summary, report

# -------------------------------
# GRADIO INTERFACE
# -------------------------------

summarizer = WebsiteSummarizer()

iface = gr.Interface(
    fn=summarizer.summarize,
    inputs=gr.Textbox(label="Website URL"),
    outputs=[
        gr.Markdown(label="ğŸ“ Website Summary"),
        gr.Markdown(label="ğŸ” Fraud Analysis Report")
    ],
    title="ğŸŒ Website Summary & Fraud Checker",
    description="Enter a website URL to generate a summary using GPT and scan for common fraud indicators."
)

iface.launch()
