# -------------------------------
# IMPORTS
# -------------------------------

import os
import re
import ssl
import socket
import whois
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse
from dotenv import load_dotenv
import openai
import gradio as gr
import time
import dns.resolver

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from dateutil import parser

# -------------------------------
# ENVIRONMENT SETUP
# -------------------------------

load_dotenv(override=True)
api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=api_key)

# -------------------------------
# WEBSITE SUMMARIZER CLASS
# -------------------------------

class WebsiteSummarizer:
    def __init__(self, model="gpt-4o"):
        self.client = client
        self.model = model

    def fetch_html_selenium(self, url):
        try:
            options = Options()
            options.add_argument("--headless=new")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            time.sleep(5)
            html = driver.page_source
            driver.quit()
            return html
        except:
            return None

    def check_ssl(self, url):
        try:
            parsed_url = urlparse(url)
            context = ssl.create_default_context()
            with socket.create_connection((parsed_url.hostname, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=parsed_url.hostname) as ssock:
                    cert = ssock.getpeercert()
            return True
        except:
            return False

    def check_domain_age(self, url):
        try:
            domain = urlparse(url).netloc or urlparse(url).hostname
            if domain.startswith("www."):
                domain = domain[4:]

            w = whois.whois(domain)
            creation = w.creation_date
            if isinstance(creation, list):
                creation = [c for c in creation if c is not None]
                if creation:
                    creation = creation[0]
                else:
                    return 0
            if isinstance(creation, str):
                creation = parser.parse(creation)
            if not isinstance(creation, datetime):
                return 0

            now = datetime.now(creation.tzinfo) if creation.tzinfo else datetime.now()
            age_months = (now.year - creation.year) * 12 + (now.month - creation.month)
            return max(0, age_months)
        except:
            return 0

    def is_url_suspicious(self, url):
        parsed = urlparse(url)
        suspicious = 0
        if re.match(r"\d{1,3}(\.\d{1,3}){3}", parsed.hostname or ""):
            suspicious += 1
        if re.search(r"(login|verify|secure|update)", parsed.hostname or ""):
            suspicious += 1
        if parsed.hostname and parsed.hostname.count('.') > 3:
            suspicious += 1
        if len(url) > 75:
            suspicious += 1
        return suspicious >= 2

    def check_redirects(self, url):
        try:
            r = requests.get(url, allow_redirects=True, timeout=5)
            return len(r.history) >= 3
        except:
            return False

    def check_security_headers(self, url):
        try:
            r = requests.head(url, timeout=5)
            headers = r.headers
            missing = []
            if 'X-Content-Type-Options' not in headers:
                missing.append('X-Content-Type-Options')
            if 'Strict-Transport-Security' not in headers:
                missing.append('Strict-Transport-Security')
            if 'Content-Security-Policy' not in headers:
                missing.append('Content-Security-Policy')
            return missing
        except:
            return ['All headers missing or unreachable']

    def check_for_hotlinked_resources(self, soup):
        hotlinks = 0
        for tag in soup.find_all(['img', 'link']):
            src = tag.get('src') or tag.get('href')
            if src and any(trusted in src for trusted in ['google.com', 'facebook.com', 'microsoft.com']):
                hotlinks += 1
        return hotlinks > 2

    def check_forms(self, soup, url):
        forms = soup.find_all('form')
        suspicious_forms = 0
        for form in forms:
            action = form.get('action')
            if action and not action.startswith('https'):
                suspicious_forms += 1
            if any(field.get('name') in ['card', 'cc', 'cvv', 'password'] for field in form.find_all(['input', 'textarea'])):
                suspicious_forms += 1
        return suspicious_forms

    def check_minimal_content(self, html_content):
        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        return len(text.split()) < 50

    def check_blacklists(self, url):
        return False  # Placeholder

    def analyze_content_with_ai(self, text):
        prompt = f"""
Analyze this website content and determine if it shows signs of being a scam or phishing website. 
Flag if there is urgency, login prompts, or poor grammar. Answer just 'safe', 'suspicious', or 'fraudulent'.

Content:
{text[:2000]}
"""
        try:
            result = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            decision = result.choices[0].message.content.lower()
            if "fraud" in decision:
                return "fraudulent"
            elif "suspicious" in decision:
                return "suspicious"
            return "safe"
        except:
            return "safe"

    def check_dns_valid(self, url):
        try:
            domain = urlparse(url).netloc or urlparse(url).hostname
            dns.resolver.resolve(domain, 'A')
            return True
        except:
            return False

    def check_javascript_behavior(self, html):
        # Simulate JavaScript warning (placeholder: flag if "onclick" or "window.location" exists)
        return bool(re.search(r'onclick|window\.location|eval\(', html))

    def check_virustotal_or_trivy(self, url):
        return "No match found (placeholder)"

    def summarize(self, url):
        html = self.fetch_html_selenium(url)
        if not html:
            return "‚ùå Failed to fetch website or website does not exist", "üö® Website does not exist or has no retrievable content."

        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        trimmed_text = text[:4000]

        try:
            chat_response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": f"Summarize the following website content:\n\n{trimmed_text}"}]
            )
            summary = chat_response.choices[0].message.content.strip()
        except:
            return "‚ùå Unable to summarize content", "üö® Website could not be summarized."

        # ---------------------
        # FRAUD CHECKS
        # ---------------------
        score = 0
        ssl_valid = self.check_ssl(url)
        domain_age = self.check_domain_age(url)
        suspicious_url = self.is_url_suspicious(url)
        redirections = self.check_redirects(url)
        blacklist_flag = self.check_blacklists(url)
        header_issues = self.check_security_headers(url)
        hotlinking = self.check_for_hotlinked_resources(soup)
        form_score = self.check_forms(soup, url)
        content_status = self.analyze_content_with_ai(trimmed_text)
        minimal_content = self.check_minimal_content(html)
        dns_valid = self.check_dns_valid(url)
        js_behavior = self.check_javascript_behavior(html)
        virustotal_scan = self.check_virustotal_or_trivy(url)

        # SCORING
        if ssl_valid: score += 1
        if domain_age > 6: score += 1
        if not suspicious_url: score += 1
        if not redirections: score += 1
        if not blacklist_flag: score += 1
        if content_status == "safe": score += 2
        if not hotlinking: score += 1
        if form_score == 0: score += 1
        if len(header_issues) <= 1: score += 1
        if dns_valid: score += 1
        if minimal_content: score -= 1

        verdict = "Likely Safe ‚úÖ" if score >= 7 else "Suspicious ‚ö†Ô∏è" if score >= 4 else "Likely Fraudulent ‚ùå"

        report = f"""
### üîç Fraud Check Report
SSL Valid: {ssl_valid}  
Domain Age: {domain_age} months  
Suspicious URL Pattern: {suspicious_url}  
Redirects >= 3: {redirections}  
Blacklisted: {blacklist_flag}  
Content Classification: {content_status}  
Hotlinked Resources: {hotlinking}  
Suspicious Forms Detected: {form_score}  
Missing Headers: {", ".join(header_issues)}  
DNS Valid: {dns_valid}  
JS Behavior Suspicious: {js_behavior}  
Minimal Content: {minimal_content}  
VirusTotal/Trivy Scan: {virustotal_scan}

üõ° Final Score: {score}/11  
üö¶ Verdict: **{verdict}**
"""
        return summary, report


# -------------------------------
# GRADIO INTERFACE
# -------------------------------

summarizer = WebsiteSummarizer()

iface = gr.Interface(
    fn=summarizer.summarize,
    inputs=gr.Textbox(label="Website URL"),
    outputs=[
        gr.Markdown(label="üìù Website Summary"),
        gr.Markdown(label="üîç Fraud Analysis Report")
    ],
    title="üåê Website Summary & Fraud Checker",
    description="Enter a website URL to summarize and detect possible fraud indicators including SSL, domain age, DNS, JS behavior, minimal content, and more."
)

iface.launch()
